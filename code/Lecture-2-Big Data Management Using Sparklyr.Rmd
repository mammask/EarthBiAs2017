---
title: "Big data management using `sparklyr` <br>"
author: "Kostas Mammas, Statistical Programmer <br> mail mammaskon@gmail.com <br>"
date: "EarthBiAs2017, Rhodes Island, Greece"
output:
  github_document:
    toc: true
    toc_depth: 3
always_allow_html: yes
---

<style type="text/css">

body{ /* Normal  */
font-size: 14px;
}
td {  /* Table  */
font-size: 12px;
}
h1 { /* Header 1 */
font-size: 24px;
color: DarkBlue;
}
h2 { /* Header 2 */
font-size: 22px;
color: DarkBlue;
}
h3 { /* Header 3 */
font-size: 18px;
color: DarkBlue;
}
code.r{ /* Code block */
font-size: 12px;
}
pre { /* Code block */
font-size: 12px
}

</style>

# Introduction to `sparklyr`

**Apache Spark** is an open source parallel processing framework for running large-scale data analytics applications across clustered computers. It can handle both batch and real-time analytics and data processing workloads.

**sparklyr** is an R interface to Apache Spark, a fast and general engine for big data processing. This package supports connecting to local and remote Apache Spark clusters, provides a 'dplyr' compatible back-end, and provides an interface to Spark's built-in machine learning algorithms

## Installation - Local Remote Apache Spark cluster

As a first step you need to install **sparklyr** package from CRAN as follows:

```{r,eval=FALSE}
# Install sparklyr package
install.packages("sparklyr")
```

You need to install also **spark** to set up a Local Remote Apache Spark cluster:

```{r,eval=FALSE}
# Load spark
library("sparklyr")
# Obtain spark available versions
allVer <- spark_available_versions()
# Obtain last version
latVer <- allVer[nrow(allVer),"spark"]
# Install last version of spark
spark_install(version = latVer)
```

## Useful functions
