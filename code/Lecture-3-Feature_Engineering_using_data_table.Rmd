---
title: "Feature Engineering using `data.table`"
author: "Giorgos Kaiafas, PhD Researcher <br> mail: <georgios.kaiafas@uni.lu> <br> Kostas Mammas, Statistical Programmer <br> mail: <mammaskon@gmail.com> <br>"
date: "EarthBiAs2017, Rhodes Island, Greece"
output:
  github_document:
    toc: true
    toc_depth: 4
always_allow_html: yes
---


##Introduction

### Lecture workflow

The purpose of this session is to fit a Machine Learning model to predict daily rainfall occurence using a set of environmental variables.

We need to answer a set of questions before we start working on this task:

1.  What type of information will be used?
2.  How are we going to access this information?
3.  What type of data cleaning techniques will be used in order to perform feature transformation?
4.  Which environmental variables will be used in our modelling approach?
5.  If we select 3 meteorological variables how are we going to gather this information in order to fita model to a particular meteorological station?

In order to answer these questions we will follow the steps bresented below:

1.  Obtain environmental daily rainfall records from ECA&D using the `ECADownloader` tool
2.  Select variables of interest
3.  Perform data cleaning/ transofrmation techniques using the `data.table` package
4.  Select country/ stations that will be used to perform feature engineering
5.  Find optimal period where a station has the available records for all the selected meteorological variables.

### ECA&D

ECA&D is a European Climate Assessment & Dataset project. Presented is information on changes in weather and climate extremes, as well as the daily dataset needed to monitor and analyse these extremes. ECA&D was initiated by the ECSN in 1998 and has received financial support from the EUMETNET and the European Commission. You can access the ECA&D website using the following [link](http://www.ecad.eu/).

## Download environmental data using the `ECADownloader` tool

The `ECADownloader` is a set of functions built in `R` using `data.table` and provide functionalities when it comes to download specific daily environmental series from all the available meteorological stations in Europe.

Using the following [link](https://github.com/mammask/ECADownloader) you can use the `ECADownloader` and obtain the environmental indices of interest.

``` {r, EVAL = TRUE}
# Load packages
library(data.table)
library(stringr)
library(tcltk)

# Define Path
currPath <- "~/Documents/EarthBiAs2017"

# Set current working directory
setwd(currPath)

# Define if dataset will be blended or nonblended
blended   <- TRUE  

# Define variables to transform
metVarTbl <- data.table(metVar = c("DailyMaxTemp", "DailyMinTemp",
                                   "DailyMeanTemp", "DailyPrecipAmount",
                                   "DailyMeanSeaLVLPress","DailyCloudCover",
                                   "DailyHumid", "DailySnowDepth", 
                                   "DailySunShineDur","DailyMeanWindSpeed",
                                   "DailyMaxWindGust","DailyWindDirection"),
                        Include = c("yes", "yes","yes","yes","no","yes","yes",
                                    "no","yes","yes","no","no"))

# Keep selected
metVarTbl <- metVarTbl[Include=="yes", metVar]
print(metVarTbl)
```

In the next step we need to source the supporting functions script available in the ECADownload tool:

``` {r, EVAL = TRUE }
# Load supporting functions
source("./externalFunctions_v1a.R")
```

### Availability of stations

The following function provides information about the list of the available meteorological stations per environmental variable of interest:

``` {r, EVAL = TRUE }
# Function to otain list of stations having the specific environmental variable
availStatPerVar <- function(metVarID){
  
  # function name: availStatPerVar
  #         input: metVarID - A vector indicating the meteorological variables of interest
  #                (see ECADownloader for naming conventions)
  #        output: availableStationsMap a data.table with the available stations
  
  # Define the download path 
  tempDownloadPath <- paste0(currPath,"/",metVarID,"/")
  # File with the list of stations for the specific meteorological variable
  precipStat  <-  paste0(currPath,"/",metVarID,"/stations.txt")
  # Reaf stations file - skipping first 16 rows
  tempMapping <- data.table(read.table(precipStat, skip = 16 ,sep=",",
                                       stringsAsFactors = FALSE, header=TRUE, quote=""))
  
  # Manipulate Longitude and Lattitude - Convert Degrees,minutes,seconds to decimal degrees 
  tempMapping[,latUpd:=sapply(LAT,findLoc)][,longUpd:=sapply(LON,findLoc)]
  tempMapping[,LatLong:=paste0(latUpd,":",longUpd)]
  tempMapping <- tempMapping[!duplicated(LatLong)] # get rid off stations with duplicated locations
  
  # Find Available Stations with their names and their location
  dataFileNames <- list.files(tempDownloadPath, pattern=".txt")
  # List of files with rainfall records
  dataFileNames <- dataFileNames[!dataFileNames %in% c("elements.txt", "sources.txt","stations.txt")]
  dataFileNamesIDs <- as.numeric(gsub(".txt","",
                                      gsub(paste0(linkMap[VarName==metVarID,ID],
                                                  "_STAID"),"",dataFileNames)
                                      )
                                 )
  # data.table with stations
  availableStationsMap <- data.table(STAID=dataFileNamesIDs)
  availableStationsMap <- merge(availableStationsMap,
                                tempMapping[,c("STAID",
                                               "STANAME",
                                               "CN",
                                               "latUpd",
                                               "longUpd",
                                               "LatLong"), with=F],
                                by=c("STAID"), all.y=TRUE)
  availableStationsMap[,MetVar:=metVarID]
  
  cat("Available data of",metVarID," for ",length(unique(availableStationsMap$CN)),"countries\n")
  
  return(availableStationsMap)
}
```

We can find the list of stations having daily precipitation rainfall records:

``` {r, EVAL = TRUE }
resTbl <- availStatPerVar(metVarID = "DailyPrecipAmount")
print(resTbl)
```

This result can be extended to find the list of stations for the following environmental variables:

1.  DailyMaxTemp
2.  DailyMinTemp
3.  DailyMeanTemp
4.  DailyPrecipAmount
5.  DailyCloudCover
6.  DailyHumid
7.  DailySunShineDur
8.  DailyMeanWindSpeed

``` {r, EVAL = TRUE }
datList <- list()
k <- 1
for (metVarID in metVarTbl){
  
  print(paste0("Processing:",metVarID,"\n"))
  datList[[k]] <- availStatPerVar(metVarID)
  k <- k + 1 
}
overall <- rbindlist(datList)
```

## Selection of environmental variables for analysis


We will use the following environmental variables:

1.  Daily Precipitation Amount
2.  Daily Mean Temperature
3.  Daily Mean Humidity

## Challenges regarding data availability

One of the main challenges is whether a station has information about the 3 variables of interest. In this section we will identify which stations have this information:

1.  Find stations with rainfall and precipitation records:

``` {r, EVAL = TRUE }
# Find station with Rainfall and Temperature Records
rainMeanTemp <- overall[ MetVar == "DailyPrecipAmount",
                         .(STAID)][ ,overall[ MetVar == "DailyMeanTemp",
                                       .(STAID)],
                             on = .(STAID = STAID), nomatch = 0L]
```

2.  Find stations with Rainfall, Temperature and Humidity Records:

``` {r, EVAL = TRUE }
# Find stations with Rainfall, Temperature and Humidity records
rainMeanTempHumid <- rainMeanTemp[, overall[ MetVar == "DailyHumid", .(STAID)], 
                                  on = .(STAID = STAID), nomatch = 0L]

rainMeanTempHumid
```

3.  Find the country with the most stations having the 3 environmental variables

``` {r, EVAL = TRUE }
allStations <- unique(overall[STAID %in% 
                                rainMeanTempHumid[, STAID] &
                                MetVar %in% c("DailyPrecipAmount",
                                              "DailyMeanTemp",
                                              "DailyHumid")][,.(STAID,CN)])

# Select Spanish Stations
stationsToKeep <- allStations[CN == "ES", STAID]
```

You can visit the following [link](https://github.com/mammask/EarthBiAs2017/blob/master/Lecture-1-Data_Management.md) in order to improve your skills in `data.table` merge.

## Data Loading in R

At this point we have dowloaded the data locally. As a next step we need to load the desired variables in R. For this reason we will run the following function:

``` {r, EVAL = TRUE }
# Obtain data in R using as input a set of environmental variables:
obtainStationDat <- function(linkMap, metVarId, stationsToKeep){
  
  library(data.table)
  library(stringr)
  library(tcltk)
  
  # Create an empty list
  tempStat <- list()
  # Obtain download path
  tempDownloadPath <- linkMap[VarName == metVarId,downloadDatPath]
  # Obtain files with the records
  dataFileNames <- list.files(tempDownloadPath, pattern=".txt")
  # Remove un-necessary stations 
  dataFileNames <- dataFileNames[!dataFileNames %in% 
                                   c("elements.txt", "sources.txt","stations.txt")]
  
  # Create a data.table with the names of the files of the rainfall records
  dataFileNamesMap <- data.table(FileName=dataFileNames)
  # Create a data.table with the names of the stations
  stationsToKeep <- data.table(Filename = stationsToKeep)
  # Create the names of the records of interest (see pattern of file names)
  stationsToKeep[, Filename := paste0(linkMap[VarName == metVarId, ID],"_STAID",
                                      str_pad(string = Filename,
                                              width = 6,
                                              side = "left",
                                              pad = "0"),".txt")]
  
  # Define files to download
  statToDownload <- dataFileNamesMap[FileName %in% stationsToKeep[,Filename], FileName]
  
  # Create a progress bar so as to have a sense of the progress
  k <- 1
  mypb <- tkProgressBar(title ="Percentage Complete: 0%", min=0,
                        max=length(statToDownload), initial=0, width=400)
  
  # For loop to read the records of each meteorological station
  for (tempFile_id in statToDownload){
    setTkProgressBar(mypb, k, title=paste0("Percentage Complete: ",
                                           round(k/length(statToDownload)*100,digits = 1),
                                           "% ","Processing Data Read..."))
    tempStat[[k]] <- data.table(read.table(paste0(tempDownloadPath,"/",tempFile_id)
                                           ,skip=20, sep=",",header=TRUE, stringsAsFactors=FALSE))
    
    # Format the date
    tempStat[[k]][,DATE:=as.Date(as.character(DATE), format("%Y%m%d"))]
    # Compute year variable
    tempStat[[k]][,Year:= year(DATE)]
    # Compute month variable
    tempStat[[k]][,Month:= month(DATE)]
    # Create a name with the id of the meteorological variable of interest
    tempStat[[k]][,VarName:=metVarId]
    
    k <- k + 1
  }
  close(mypb)
  return(rbindlist(tempStat))
}
```

We can save time by running this function using a parallel processing approach:

``` {r, EVAL = TRUE }
library(foreach)
library(doParallel)

cl <- makeCluster(3)
registerDoParallel(cl)

# Read on parallel the available stations
allRes <- foreach(metVarId = c("DailyPrecipAmount",
                               "DailyMeanTemp",
                               "DailyHumid"), .verbose = T) %dopar% obtainStationDat(linkMap,
                                                                                      metVarId,
                                                                                      stationsToKeep
                                                                                     )
stopCluster(cl)
allRes <- rbindlist(allRes)
gc()
```

## A heuristic approach to impute missing values

There are several approaches to deal with the problem of missing values. These methods refer to multiple imputation techiques, deleting records with missing values, replacing with mean or median etc.

In this exercise we impute maximum 5 records per month using the monthly mean. In cases where more than 5 records appear during a month, these records are excluded.

``` {r, EVAL = TRUE }
# Perform a data cleaning process in order to impute max 5 records per month
# Replace values -9999 with NA
allRes[RR == -9999, RR:= NA]

# Compute Monthly Mean by Station month and environmental variable
averageMapper <- allRes[, .(MonthlyMean = mean(RR, na.rm = T)), by = c("STAID","Year", "Month", "VarName")]

# Find number of missing records per station month and meteorological varible 
countNA  <- allRes[is.na(RR), .(CountNumNAs = .N), by =  c("STAID","Year", "Month", "VarName")]

# Obtain count for each monthly mean
resultDT <- merge(averageMapper,countNA, all.x = T, by = c("STAID","Year", "Month", "VarName"))

# Obtain results to original dataset
allRes <- merge(allRes,resultDT, all.x = T, by = c("STAID","Year", "Month", "VarName"))
rm(averageMapper)
rm(countNA)
rm(resultDT)
gc()

# Replace missing recods with mean 
allRes[, c("MonthlyMean","RR"):= lapply(.SD, as.numeric), .SDcols = c("MonthlyMean","RR")]

allRes[is.na(RR) & !is.na(CountNumNAs) & CountNumNAs <= 5, RR:= MonthlyMean]

# Ommit observations with more than 5 NAs during a monthly period
allRes <- allRes[is.na(CountNumNAs) | CountNumNAs <= 5]

# Find representtive stations with 3 meteorological variables
repStat <- allRes[, .N, by = list(STAID, DATE)][N>2][order(-N)]

# Update dataset by keeping stations with 3 available meteorological records
allRes  <- allRes[repStat, on = .(STAID = STAID, DATE = DATE)]

# Transform to wide format
allRes <- data.table::dcast.data.table(allRes, STAID + DATE + Year + Month ~ VarName, value.var = c("RR", "Q_RR"))
```

## Find the optimal period length of consecutive daily records

At this stage, data imputation is complete. Each station has clean and imputed data. However, we need to ensure that we have complete records. For example, some stations might have discontinuations and two consecutive records might be apart for more than one day.

In the following script we compute the length of each stations period with consecutive rainfall records:

``` {r, EVAL = TRUE }
# Find period with maximum number of available meteorological records
allRes <- copy(allRes[order(STAID, DATE)])
allRes[, Cons := as.numeric(DATE - shift(x = DATE, n = 1, type = "lag", fill = NA))-1, by = STAID]
allRes[is.na(Cons), Cons:= 0]
allRes[, Group := cumsum(Cons), by = STAID]

# Obtain period length for each station
periodTbl <- allRes[, { minDate = min(DATE);
                        maxDate = max(DATE);
                        
                        list( MinDate = minDate, 
                              MaxDate = maxDate,
                              Length  = as.numeric(maxDate-minDate))
                        },
                        by = list(STAID, Group)
                    ][order(STAID, -Length)]
```


### Obtain records for the maximum period of consecutive rainfall

For each station we find the maximum time period of consecutive rainfall.

```{r, results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(data.table)
library(zoo)
library(lubridate)
library(tidyverse)
library(RcppRoll)
library(caTools)
```

```{r, EVAL = TRUE}
maxLenths <- periodTbl[, .(Length = max(Length)), by = STAID]
maxDuration <- periodTbl[maxLenths, on = .(STAID = STAID, Length = Length)]
head(maxDuration)
```

```{r, EVAL = TRUE}
allRes <- merge(allRes, maxDuration[, !"Group", with=FALSE], all.x = T, by = "STAID")
allRes <- allRes[DATE %between% list(MinDate, MaxDate)]
```
## Feature engineering

In this section we begin the feature engineering approach.  
Feature engineering is the process of using domain knowledge of the data to create features that make the machine learning algorithms work. Rolling analysis will be a part of the process.   
*```Coming up with features is difficult, time-consuming, requires expert knowledge. "Applied machine learning" is basically feature engineering.```*    
*```Andrew Ng, Machine Learning and AI via Brain simulations```*   

A common technique to assess the constancy of a modelâ€™s parameters is to compute parameter estimates over a rolling window of a fixed size through the sample.   
Moving average methods are common in rolling analysis, and these methods lie at the heart of the technical analysis of financial time series. Moving averages typically use either equal weights for the observations or customized declining weights.   
We create the new features by using rolling averages with and without weighs.


### Rolling Mean

First, we calculate the average daily series of the previous 30 and 60 available days for the daily rainfall, humidity and temperature.

```{r, EVAL = TRUE}
# Compute Quarter indicator variable
allRes[, Quarter:= quarter(DATE)]

## Computation of rolling mean using as time window 30 days
# ---------------------------------------------------------

# As a pre-processing step we have to exclude the stations that have less than 30 observations
excludeStations30 <- allRes[, .N, by=STAID][N<30, STAID]
allRes <- allRes[!(STAID %in% excludeStations30)]

# Provide names of the variables to create
varsToCreate30 <- c("PrecipRollAverage30","HumidRollAverage30","TempRollAverage30")

# Provide list of the variables that will be used to compute rolling mean
varsToModify <- c("RR_DailyPrecipAmount", "RR_DailyHumid", "RR_DailyMeanTemp")

# Compute rolling mean using a time window of 30 days
# align is mandatory in order to calculate for each observation the rolling mean of the 30 previous values.
allRes[, c(varsToCreate30) := lapply(.SD, rollmean, 30, fill = NA, align = "right"),.SDcols = varsToModify, by = STAID]

## Computation of rolling mean using as time window 60 days
# ---------------------------------------------------------

# As a pre-processing step we have to exclude the stations that have less than 60 observations
excludeStations60 <- allRes[, .N, by=STAID][N<60, STAID]
allRes <- allRes[!(STAID %in% excludeStations60)]

# Provide names of the variables to create
varsToCreate60 <- c("PrecipRollAverage60","HumidRollAverage60","TempRollAverage60")

# Provide list of the variables that will be used to compute rolling mean
varsToModify <- c("RR_DailyPrecipAmount", "RR_DailyHumid", "RR_DailyMeanTemp")

# Compute rolling mean using a time window of 60 days
# align is mandatory in order to calculate for each observation the rolling mean of the 60 previous values.
allRes[, c(varsToCreate60) := lapply(.SD, rollmean, 60, fill = NA, align = "right"),.SDcols = varsToModify, by = STAID]
str(allRes)

```

### Lag

The lag of each one of the available meteorological variables is calculated to capture possible daily dependencies. In the following script we calculate lags for the five previous days:

```{r, EVAL = TRUE}
## Computation of daily lag variables t-1 up to t-5
# ---------------------------------------------------------

# Provide names of the variables to create
varsToCreateLag <- c("Preciplag", "Humidlag", "Templag")

# Provide list of the variables that will be used to compute rolling mean
varsToModify    <- c("RR_DailyPrecipAmount", "RR_DailyHumid", "RR_DailyMeanTemp")

# Perform a for loop to compute variables for each lag
for (i in 1:5){
  print(paste0("Computing step ",i,"\n"))
  
  allRes[, paste0(varsToCreateLag,i):= lapply(.SD, data.table::shift,
                                           type = "lag",
                                           n    = i,
                                           fill = NA
                                           ), 
                .SDcols = varsToModify,
      by = STAID
      ]
}
```

### Rolling Weighted Mean

In this section, we compute a weighted rolling mean so us to give more weight to the more recent variables:

```{r, EVAL = TRUE}
## Computation of rolling weighted mean using as time window 15 days
# ---------------------------------------------------------

# Create a custom vector of weights
customWeights <- 1:15/30

# Provide names of the variables to create
varsToCreateWeightRollMean <- c("RainweightedRollMean15",
                                "TempweightedRollMean15",
                                "HumidweightedRollMean15")

# Provide list of the variables that will be used to compute the weighted rolling mean
varsToModify               <- c("RR_DailyPrecipAmount", "RR_DailyHumid", "RR_DailyMeanTemp")


# Compute weighted rolling mean for the previous 15 days of each record per station
allRes[, c(varsToCreateWeightRollMean):= lapply(.SD,
                                            roll_mean,
                                            n       = 15L,
                                            weights = customWeights,
                                            fill    = NA,
                                            align   = "right"),
    .SDcols = varsToModify,
    by=STAID
    ]

```

### Rolling Standard Deviation

The same logic can be applied to compute the rolling standard deviation:

```{r, EVAL = TRUE}
## Computation of rolling sd using as time window 15 days
# ---------------------------------------------------------

# Provide names of the variables to create
varsToCreateWeightRollSD <- c("RainweightedRollSd15",
                              "TempweightedRollSd15",
                              "HumidweightedRollSd15")

# Provide list of the variables that will be used to compute the weighted rolling sd
varsToModify               <- c("RR_DailyPrecipAmount", "RR_DailyHumid", "RR_DailyMeanTemp")


# Compute weighted rolling sd for the previous 15 days of each record per station
allRes[, (varsToCreateWeightRollSD):= lapply(.SD,
                                            roll_sd,
                                            n       = 15L,
                                            weights = customWeights,
                                            fill    = NA,
                                            align   = "right"),
    .SDcols = varsToModify,
    by=STAID
    ]
```

### Rolling Quantile 

```{r, EVAL = TRUE}
## Computation of rolling quantile (90%) using as time window 15 days
# ---------------------------------------------------------

allRes[, RainRollQuantile15:=  caTools::runquantile(RR_DailyPrecipAmount, k = 15, 
                                                    probs = 0.9, endrule="NA", align = "right"), by = STAID]
allRes[, TempRollQuantile15:=  caTools::runquantile(RR_DailyMeanTemp, k = 15, 
                                                    probs = 0.9, endrule="NA", align = "right"), by = STAID]
allRes[, HumidRollQuantile15:=  caTools::runquantile(RR_DailyHumid, k = 15, 
                                                     probs = 0.9, endrule="NA", align = "right"), by = STAID]
str(allRes)
```

